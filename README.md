# DS & AI Internship : This repository contains the work completed during my Data Science & Artificial Intelligence Internship.
# Day 1 â€“ Introduction & Setup
# Tasks Completed
-Git & GitHub setup
-Repository initialization
-Pushing project structure to GitHub
-Writing and running a Hello World program in Python

# Day 2 â€“ Python Basics
# Tasks Completed
-Understanding Python data types
-Using input() for user interaction
-Type casting using int() and float()
-Basic arithmetic operations
# Simple programs:
-Age calculator (future age prediction)
-Bill splitting using division
-Item receipt program using variables and formatting

# Day 3 â€“ Python Data Structures
# Tasks Completed
-Creating and managing lists
-Using append(), remove(), and sort()
-Zero-based indexing and list slicing
-Working with tuples and understanding immutability
-Creating and updating dictionaries
-Safe dictionary access using .get()
-Using sets and performing union, intersection, and difference
# Simple programs:
-Inventory management system
-Temperature readings analysis
-Screen resolution immutability example
-Digital rolodex using dictionaries
-Interest comparison using set operations

# Day 4 â€“ Control Flow
# Tasks Completed
-Using if, elif, and else statements
-Comparison and logical operators
-Using while loops and for loops
-Using break and continue statements
-Combining loops with user input
# Simple programs:
-Menu-driven calculator
-Repeated input with exit condition
-Greeting program with loop

# Day 5 â€“ Functions & Modular Programming
# Tasks Completed
- Learned how to define and call functions in Python.
- Practiced passing arguments and returning values from functions.
- Implemented reusable logic by separating code into multiple Python files.
- Created utility functions in separate modules and imported them where required.
- Organized the program structure using files such as `main.py`, `math_operations.py`, and `utils.py`.
# Outcome
- Understood the importance of modular programming for clean and maintainable code.
- Improved code reusability and readability by separating logic into functions and modules.
- Gained practical experience in building multi-file Python programs.

# Day 6 â€“ Functions & Custom Modules
# Tasks Completed
- Implemented a program to calculate area and perimeter of geometric shapes using functions and return values.
- Practiced defining reusable functions and calling them with appropriate arguments.
- Created a custom logic module to organize commonly used functions.
- Imported and reused functions from the custom module in other Python files.
# Outcome
- Gained a strong understanding of functions, parameters, and return values.
- Learned how modular programming improves code reusability and readability.
- Built practical experience in creating and using custom Python modules.

# Day 7 â€“ File Handling (Text, CSV & Excel)
# Tasks Completed
- Practiced reading from and writing to text files using Python.
- Worked with CSV files to store and retrieve structured data.
- Read and processed data from Excel (`.xlsx`) files.
- Implemented file handling programs using multiple file formats.
# Outcome
- Gained hands-on experience with Python file I/O operations.
- Learned to handle different data formats commonly used in real-world applications.
- Improved confidence in managing external data sources using Python.

# Day 8 â€“ NumPy & Numerical Computing
# Tasks Completed
-Created NumPy arrays and worked with multi-dimensional data
-Applied broadcasting and vectorization for efficient computations
-Normalized datasets using axis-based statistical operations
-Reshaped and transposed arrays to prepare ML-ready data
-Performed matrix operations such as dot products
# Outcome
-Understood NumPy broadcasting and vectorization concepts
-Learned how array reshaping is used in machine learning models
-Gained practical experience with numerical and linear algebra operations
-Improved efficiency in handling large numerical datasets

# Day 9 â€“ Pandas Series
# Tasks Completed
-Created a product price catalog using Series and indexing.
-Detected missing values, filled them, and filtered grades using boolean masking.
-Cleaned usernames using vectorized string methods (strip(), lower(), contains()).
# Outcome
-Learned label-based and positional indexing
-Applied boolean filtering techniques
-Managed missing data using .isnull() and .fillna()
-Used vectorized string operations for text processing

# Day 10 â€“ Data Cleaning
# Tasks Completed
-Identified and handled missing values in datasets
-Detected and removed duplicate records
-Performed data type conversion using .astype() and pd.to_numeric()
-Cleaned and standardized categorical string data
-Validated dataset consistency after cleaning
# Outcome
-Learned to prepare real-world datasets for analysis
-Improved data integrity through validation techniques
-Strengthened practical understanding of data preprocessing
-Built confidence in handling messy datasets

# Day 11 - Data Visualization
# Tasks Completed
-Created line plots using Matplotlib
-Built scatter plots for relationship analysis
-Designed bar charts for categorical comparisons
-Customized plots using titles, labels, legends, and grids
-Implemented subplots for multiple visualizations in one figure
# Outcome
-Visualized trends and patterns effectively
-Analyzed relationships between variables
-Improved presentation of data insights
-Gained hands-on experience in creating professional data visualizations

# Day 12 â€“ Dashboard Development
# Tasks Completed
-Developed an interactive data dashboard using Python
-Integrated multiple visualizations into a single interface
-Loaded and processed datasets for analysis
-Applied data cleaning before visualization
-Organized code using modular files (day12fundamentals.py, dashboard.py)
-Structured charts for clear insight presentation
# Outcome
-Learned how to combine data analysis and visualization in one project
-Strengthened dashboard-building and presentation skills
-Improved ability to transform raw data into meaningful insights
-Gained hands-on experience with real-world analytical workflows
-Enhanced understanding of modular project structure

# Day 13 â€“ Exploratory Data Analysis (EDA)
# Tasks Completed
-Performed Exploratory Data Analysis on structured datasets
-Conducted univariate analysis to understand feature distributions
-Applied bivariate analysis to examine relationships between variables
-Calculated correlation between numerical features
-Identified and analyzed outliers using statistical methods
-Visualized distributions and relationships using plots
# Outcome
-Gained a strong understanding of the EDA workflow
-Learned how to interpret data distributions effectively
-Identified relationships and patterns between variables
-Detected multicollinearity using correlation analysis
-Improved ability to recognize and handle outliers
-Strengthened analytical thinking before model development

# Day 14 â€“ Feature Engineering
# Tasks Completed
-Applied Label Encoding and One-Hot Encoding
-Implemented feature scaling techniques
-Generated polynomial features
-Prepared optimized datasets for modeling
-Evaluated transformation impact on performance
# Outcome
-Learned importance of feature preparation in ML
-Improved dataset optimization skills
-Understood how transformations affect model accuracy
-Strengthened readiness for machine learning model building

# Technologies Used
-Python
-Git
-GitHub
# Learning Outcomes
-Learned how to manage repositories using GitHub
-Gained hands-on practice with Python fundamentals
-Improved understanding of data types and input handling
-Learned Python data structures and control flow

ðŸ‘¤ Author
Achinthya M
Data Science & AI Intern
